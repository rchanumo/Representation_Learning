{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "from math import ceil\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import regex\n",
    "import pickle\n",
    "import time\n",
    "import tables\n",
    "import xml.etree.ElementTree as et\n",
    "import subprocess\n",
    "import imblearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cTakes_vectors\n",
    "from icd9cms import search as search_d\n",
    "from icd9pcs import search as search_p\n",
    "\n",
    "#read choi et al embeddings file\n",
    "codes = np.loadtxt('./Data/embeddings/claims_codes_hs_300.txt', skiprows=2, usecols=0, dtype=str)\n",
    "embeddings = np.loadtxt('./Data/embeddings/claims_codes_hs_300.txt', skiprows=2, usecols=list(range(1,301)))\n",
    "\n",
    "diagnoses_mlb = pickle.load(open('./Data/diagnoses_files/diagnoses_label_encoder.pkl', 'rb'))\n",
    "procedure_mlb = pickle.load(open('./Data/procedure_files/procedure_label_encoder.pkl', 'rb'))\n",
    "\n",
    "embeddings_dict = dict()\n",
    "for i in range(len(codes)):\n",
    "    if(codes[i].startswith('IPR_')):\n",
    "        embeddings_dict[codes[i][4:]] = embeddings[i]\n",
    "    if(codes[i].startswith('IDX_')):\n",
    "        embeddings_dict[codes[i][4:]] = embeddings[i]\n",
    "        \n",
    "#find embeddings for non-leaf codes in filtered diagnoses codes\n",
    "for code in tqdm(diagnoses_mlb.classes_):\n",
    "    node = search_d(code)\n",
    "    if(not node.is_leaf):\n",
    "        leaves_subset = []\n",
    "        for leaf_node in node.leaves:\n",
    "            leaves_subset.append(leaf_node.alt_code)\n",
    "        leaf_embeddings = [embeddings_dict[leaf_code] for leaf_code in leaves_subset if embeddings_dict.get(leaf_code) is not None]\n",
    "        final_embedding = sum(leaf_embeddings)/len(leaf_embeddings)\n",
    "        embeddings_dict[code] = final_embedding\n",
    "\n",
    "#find embeddings for non-leaf codes in filtered procedure codes\n",
    "for code in tqdm(procedure_mlb.classes_):\n",
    "    node = search_p(code)\n",
    "    if(not node.is_leaf):\n",
    "        leaves_subset = []\n",
    "        for leaf_node in node.leaves:\n",
    "            leaves_subset.append(leaf_node.alt_code)\n",
    "        leaf_embeddings = [embeddings_dict[leaf_code] for leaf_code in leaves_subset if embeddings_dict.get(leaf_code) is not None]\n",
    "        final_embedding = sum(leaf_embeddings)/len(leaf_embeddings)\n",
    "        embeddings_dict[code] = final_embedding\n",
    "\n",
    "#get embeddings for medicines using fasttex\n",
    "medicine_mlb = pickle.load(open('./Data/medicine_files/medicine_label_encoder.pkl', 'rb'))\n",
    "model = fasttext.load_model('./Data/word_models/BioWordVec_PubMed_MIMICIII_d200.bin')\n",
    "for medicine_name in medicine_mlb.classes_:\n",
    "    embeddings_dict[medicine_name] = model.get_word_vector(medicine_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_mlb = pickle.load(open('./Data/diagnoses_files/diagnoses_label_encoder.pkl', 'rb'))\n",
    "diagnoses_label_dict = pickle.load(open('./Data/diagnoses_files/diagnoses_label_dict.pkl', 'rb'))\n",
    "procedure_label_dict = pickle.load(open('./Data/procedure_files/procedure_label_dict.pkl', 'rb'))\n",
    "medicine_label_dict = pickle.load(open('./Data/medicine_files/medicine_label_dict.pkl', 'rb'))\n",
    "\n",
    "ctakes_vectors_d = dict()\n",
    "for filename in diagnoses_label_dict.keys():\n",
    "    embeddings_list = [embeddings_dict[icd_code] for icd_code in diagnoses_mlb.inverse_transform(np.expand_dims(diagnoses_label_dict[filename],0))[0] if embeddings_dict.get(icd_code) is not None]\n",
    "    if(embeddings_list != []):\n",
    "        ctakes_vectors_d[filename] = sum(embeddings_list)/len(embeddings_list)\n",
    "ctakes_vectors_p = dict()\n",
    "for filename in procedure_label_dict.keys():\n",
    "    embeddings_list = [embeddings_dict[icd_code] for icd_code in procedure_mlb.inverse_transform(np.expand_dims(procedure_label_dict[filename],0))[0] if embeddings_dict.get(icd_code) is not None]\n",
    "    if(embeddings_list != []):\n",
    "        ctakes_vectors_p[filename] = sum(embeddings_list)/len(embeddings_list)\n",
    "ctakes_vectors_m = dict()\n",
    "for filename in medicine_label_dict.keys():\n",
    "    embeddings_list = [embeddings_dict[medicine_name] for medicine_name in medicine_mlb.inverse_transform(np.expand_dims(medicine_label_dict[filename],0))[0] if embeddings_dict.get(medicine_name) is not None]\n",
    "    if(embeddings_list != []):\n",
    "        ctakes_vectors_m[filename] = sum(embeddings_list)/len(embeddings_list)\n",
    "\n",
    "ctakes_vectors = dict()\n",
    "files_no_features = []\n",
    "for filename in ctakes_vectors_d.keys():\n",
    "    if((ctakes_vectors_p.get(filename) is not None) and (ctakes_vectors_m.get(filename) is not None)):\n",
    "        ctakes_vectors[filename] = np.hstack((ctakes_vectors_d[filename],ctakes_vectors_p[filename],ctakes_vectors_m[filename]))\n",
    "    else:\n",
    "        files_no_features.append(filename)\n",
    "with open('./Data/ctakes_vectors.pkl', 'wb') as handle:\n",
    "    pickle.dump(ctakes_vectors, handle)\n",
    "with open('./Data/ctakes_files_no_features.pkl', 'wb') as handle:\n",
    "    pickle.dump(files_no_features, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses_mlb = pickle.load(open('./Data/diagnoses_files/diagnoses_label_encoder.pkl', 'rb'))\n",
    "procedure_mlb = pickle.load(open('./Data/procedure_files/procedure_label_encoder.pkl', 'rb'))\n",
    "medicine_mlb = pickle.load(open('./Data/medicine_files/medicine_label_encoder.pkl', 'rb'))\n",
    "diagnoses_label_dict = pickle.load(open('./Data/diagnoses_files/diagnoses_label_dict.pkl', 'rb'))\n",
    "procedure_label_dict = pickle.load(open('./Data/procedure_files/procedure_label_dict.pkl', 'rb'))\n",
    "medicine_label_dict = pickle.load(open('./Data/medicine_files/medicine_label_dict.pkl', 'rb'))\n",
    "diagnoses_embeddings = pickle.load(open('./Data/label_embeddings/diagnoses_embeddings_word_emb_train.pkl', 'rb'))\n",
    "procedure_embeddings = pickle.load(open('./Data/label_embeddings/procedure_embeddings_word_emb_train.pkl', 'rb'))\n",
    "medicine_embeddings = pickle.load(open('./Data/label_embeddings/medicine_embeddings_word_emb_train.pkl', 'rb'))\n",
    "diagnoses_embeddings_dict = {code:diagnoses_embeddings[i] for i , code in enumerate(diagnoses_mlb.classes_)}\n",
    "procedure_embeddings_dict = {code:procedure_embeddings[i] for i , code in enumerate(procedure_mlb.classes_)}\n",
    "medicine_embeddings_dict = {medicine:medicine_embeddings[i] for i , medicine in enumerate(medicine_mlb.classes_)}\n",
    "\n",
    "ctakes_vectors_d = dict()\n",
    "for filename in diagnoses_label_dict.keys():\n",
    "    embeddings_list = [diagnoses_embeddings_dict[icd_code] for icd_code in diagnoses_mlb.inverse_transform(np.expand_dims(diagnoses_label_dict[filename],0))[0] if diagnoses_embeddings_dict.get(icd_code) is not None]\n",
    "    if(embeddings_list != []):\n",
    "        ctakes_vectors_d[filename] = sum(embeddings_list)/len(embeddings_list)\n",
    "ctakes_vectors_p = dict()\n",
    "for filename in procedure_label_dict.keys():\n",
    "    embeddings_list = [procedure_embeddings_dict[icd_code] for icd_code in procedure_mlb.inverse_transform(np.expand_dims(procedure_label_dict[filename],0))[0] if procedure_embeddings_dict.get(icd_code) is not None]\n",
    "    if(embeddings_list != []):\n",
    "        ctakes_vectors_p[filename] = sum(embeddings_list)/len(embeddings_list)\n",
    "ctakes_vectors_m = dict()\n",
    "for filename in medicine_label_dict.keys():\n",
    "    embeddings_list = [medicine_embeddings_dict[medicine_name] for medicine_name in medicine_mlb.inverse_transform(np.expand_dims(medicine_label_dict[filename],0))[0] if medicine_embeddings_dict.get(medicine_name) is not None]\n",
    "    if(embeddings_list != []):\n",
    "        ctakes_vectors_m[filename] = sum(embeddings_list)/len(embeddings_list)\n",
    "\n",
    "ctakes_vectors = dict()\n",
    "files_no_features = []\n",
    "for filename in ctakes_vectors_d.keys():\n",
    "    if((ctakes_vectors_p.get(filename) is not None) and (ctakes_vectors_m.get(filename) is not None)):\n",
    "        ctakes_vectors[filename] = np.hstack((ctakes_vectors_d[filename],ctakes_vectors_p[filename],ctakes_vectors_m[filename]))\n",
    "    else:\n",
    "        files_no_features.append(filename)\n",
    "with open('./Data/ctakes_vectors_word_emb_train.pkl', 'wb') as handle:\n",
    "    pickle.dump(ctakes_vectors, handle)\n",
    "# with open('./Data/ctakes_files_no_features.pkl', 'wb') as handle:\n",
    "#     pickle.dump(files_no_features, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent2vec\n",
    "# model = fasttext.load_model('./Data/word_models/BioSentVec_PubMed_MIMICIII-bigram_d700.bin')\n",
    "import sent2vec\n",
    "model = sent2vec.Sent2vecModel()\n",
    "model.load_model('./Data/word_models/BioSentVec_PubMed_MIMICIII-bigram_d700.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Data_NEW_1/downstream_datasets/Obesity'\n",
    "# x = model.get_sentence_vector('Person has malaria')\n",
    "ds = pickle.load(open(f'{data_dir}/discharge_summaries_tokenized_clamp.pkl', 'rb'))\n",
    "sent2vec_dict = {}\n",
    "for key in tqdm(ds.keys()):\n",
    "#     for sentence in ds[key]:\n",
    "#         if len(sentence)>3:\n",
    "#             print(' '.join(sentence))\n",
    "#     print('\\n\\n\\n')\n",
    "#     continue\n",
    "    sentence_emb = [model.embed_sentence(' '.join(sentence))[0,:] for sentence in ds[key][1:-1] if len(sentence)>3]\n",
    "    if sentence_emb==[]:\n",
    "        continue\n",
    "    doc_emb = sum(sentence_emb)/len(sentence_emb)\n",
    "    sent2vec_dict[key] = doc_emb\n",
    "with open(f'./Data/downstream_datasets/Obesity/sent2vec_mod_dict.pkl', 'wb') as handle:\n",
    "    pickle.dump(sent2vec_dict, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_disease_codes = {'CHF': ['428',],\n",
    "'Hypertension': ['401-405'],\n",
    "'Obesity' : ['278'],\n",
    "'CAD': ['414'],\n",
    "'Venous Insufficiency': ['459.81'],\n",
    "'Gout': ['274.9'],\n",
    "'Gallstones':  ['574'],\n",
    "'Depression': ['296', '300', '309', '311'],\n",
    "'Asthma': ['493'],\n",
    "'Gerd': ['530.81'],\n",
    "'OA': ['715'],\n",
    "'Hypercholesterolemia':['272'],\n",
    "'Hypertriglyceridemia':['272'],\n",
    "'Diabetes': ['250'],\n",
    "'OSA': ['327.23'],\n",
    "'PVD':['443.9']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = ['id', 'text', 'Asthma', 'CAD', 'CHF', \n",
    "               'Depression', 'Diabetes', 'Gallstones', 'GERD', 'Gout', \n",
    "               'Hypercholesterolemia', 'Hypertension', 'Hypertriglyceridemia', \n",
    "               'OA', 'Obesity', 'OSA', 'PVD', 'Venous Insufficiency']\n",
    "\n",
    "df_add = dict()\n",
    "def_row = {'id':None, 'text':None, 'Asthma':'N', 'CAD':'N', 'CHF':'N', \n",
    "               'Depression':'N', 'Diabetes':'N', 'Gallstones':'N', 'GERD':'N', 'Gout':'N', \n",
    "               'Hypercholesterolemia':'N', 'Hypertension':'N', 'Hypertriglyceridemia':'N', \n",
    "               'OA':'N', 'Obesity':'N', 'OSA':'N', 'PVD':'N', 'Venous Insufficiency':'N'}\n",
    "\n",
    "diagnoses_label_dict = pickle.load(open('./Data/diagnoses_files/diagnoses_label_dict.pkl', 'rb'))\n",
    "diagnoses_mlb = pickle.load(open('./Data/diagnoses_files/diagnoses_label_encoder.pkl', 'rb'))\n",
    "for disease in add_disease_codes.keys():\n",
    "    print(disease)\n",
    "    for code in add_disease_codes[disease]:\n",
    "        x = 0\n",
    "        if('.' in code):\n",
    "            code = code.split('.')[0]\n",
    "            x = 1\n",
    "        if(code not in diagnoses_mlb.classes_):\n",
    "            continue\n",
    "        ind = diagnoses_mlb.classes_.tolist().index(code)\n",
    "#         print(code,ind)\n",
    "        for filename in diagnoses_label_dict.keys():\n",
    "            if('noteevents' in filename and diagnoses_label_dict[filename][ind] == 1):\n",
    "#                 continue\n",
    "                if(df_add.get(filename) is None):\n",
    "                    df_add[filename] = def_row.copy()\n",
    "                    df_add[filename]['id'] = filename\n",
    "                if(x == 0):\n",
    "                    df_add[filename][disease] = 'Y'\n",
    "                else:\n",
    "                    df_add[filename][disease] = 'X'\n",
    "\n",
    "df_add = pd.DataFrame.from_records(list(df_add.values()))\n",
    "df_add = df_add.set_index('id')\n",
    "df_add = df_add.fillna('X')\n",
    "with open('./Data/downstream_datasets/Obesity/df_train_add.pkl', 'wb') as handle:\n",
    "    pickle.dump(df_add, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obesity challenge\n",
    "\n",
    "def read_data(data_file, labels_file):\n",
    "    #read discharge summaries    \n",
    "    xtree = et.parse(data_file)\n",
    "    xroot = xtree.getroot()\n",
    "    rows = []\n",
    "    df_cols = ['id', 'text', 'Asthma', 'CAD', 'CHF', \n",
    "               'Depression', 'Diabetes', 'Gallstones', 'GERD', 'Gout', \n",
    "               'Hypercholesterolemia', 'Hypertension', 'Hypertriglyceridemia', \n",
    "               'OA', 'Obesity', 'OSA', 'PVD', 'Venous Insufficiency']\n",
    "\n",
    "    for docs in xroot:\n",
    "        for node in docs:\n",
    "            res = []\n",
    "            res.append(node.attrib.get(df_cols[0]))\n",
    "            res.append(node[0].text)\n",
    "            res += ['X']*16\n",
    "            rows.append({df_cols[i]: res[i] \n",
    "                         for i, _ in enumerate(df_cols)})\n",
    "\n",
    "    out_df = pd.DataFrame(rows, columns=df_cols)\n",
    "    out_df.set_index('id', inplace=True)\n",
    "\n",
    "    #read labels\n",
    "    xtree = et.parse(labels_file)\n",
    "    xroot = xtree.getroot()\n",
    "    for diseases in xroot:\n",
    "        for node in diseases:\n",
    "            disease = node.attrib.get('name')\n",
    "            for entry in node:\n",
    "                out_df.loc[entry.attrib.get('id'), disease] = entry.attrib.get('judgment')\n",
    "    return out_df\n",
    "\n",
    "df_train_1 = read_data('./Data/downstream_datasets/Obesity/obesity_patient_records_training.xml',\n",
    "                      './Data/downstream_datasets/Obesity/obesity_standoff_intuitive_annotations_training.xml')\n",
    "df_train_2 = read_data('./Data/downstream_datasets/Obesity/obesity_patient_records_training2.xml',\n",
    "                      './Data/downstream_datasets/Obesity/obesity_standoff_annotations_training_addendum_intutive.xml')\n",
    "df_train = pd.concat([df_train_1,df_train_2])\n",
    "df_test = read_data('./Data/downstream_datasets/Obesity/obesity_patient_records_test.xml',\n",
    "                   './Data/downstream_datasets/Obesity/obesity_standoff_annotations_test_intuitive.xml')\n",
    "\n",
    "xtree = et.parse('./Data/downstream_datasets/Obesity/obesity_standoff_annotations_training_addendum_CHF.xml')\n",
    "xroot = xtree.getroot()\n",
    "for diseases in xroot:\n",
    "    for node in diseases:\n",
    "        disease = node.attrib.get('name')\n",
    "        print(disease)\n",
    "        for entry in node:\n",
    "            df_train.loc[entry.attrib.get('id'), disease] = entry.attrib.get('judgment')\n",
    "            \n",
    "# for index, row in df_train.iterrows():\n",
    "#     with open(f'./Data/downstream_datasets/Obesity/discharge_summaries/Obesity_train_{index}.txt', 'w') as file:\n",
    "#         file.write(row['text'])\n",
    "# for index, row in df_test.iterrows():\n",
    "#     with open(f'./Data/downstream_datasets/Obesity/discharge_summaries/Obesity_test_{index}.txt', 'w') as file:\n",
    "#         file.write(row['text'])\n",
    "        \n",
    "with open(f'./Data/downstream_datasets/Obesity/df_train.pkl', 'wb') as handle:\n",
    "    pickle.dump(df_train, handle)\n",
    "with open(f'./Data/downstream_datasets/Obesity/df_test.pkl', 'wb') as handle:\n",
    "    pickle.dump(df_test, handle)\n",
    "\n",
    "# subprocess.call(['java', '-jar', './Data/AdditionalData/code/SentenceSplitter.jar', \n",
    "#                  './Data/downstream_datasets/Obesity/discharge_summaries/', \n",
    "#                  './Data/downstream_datasets/Obesity/sentence_split_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find primary diagnoses\n",
    "from icd9cms import search\n",
    "patient2idx = pickle.load(open('./Data/patient2idx.pkl', 'rb'))\n",
    "\n",
    "df_diagnoses_icd = pd.read_csv('./Data/MIMICIII_latest/DIAGNOSES_ICD.csv')\n",
    "df_grouped = df_diagnoses_icd.groupby(['SUBJECT_ID','HADM_ID'])\n",
    "primary_diagnoses_dict = {}\n",
    "diagnoses_label_dict = pickle.load(open('./Data/diagnoses_files/diagnoses_dict_ctakes.pkl', 'rb'))\n",
    "diagnoses_mlb = pickle.load(open('./Data/diagnoses_files/diagnoses_label_encoder.pkl', 'rb'))\n",
    "diagnoses_classes = diagnoses_mlb.classes_\n",
    "for patient_info, df_subset in df_grouped:\n",
    "    df_subset = df_subset.sort_values('SEQ_NUM')\n",
    "    icd_codes = df_subset['ICD9_CODE'].values.tolist()\n",
    "    if(type(icd_codes[0]) != float and patient2idx.get(patient_info) is not None):\n",
    "        primary_icd = icd_codes[0]\n",
    "        parent = search(primary_icd).ancestors()[-1]\n",
    "        if(parent in diagnoses_label_dict[f'noteevents_{patient2idx[patient_info]}.txt']):\n",
    "            primary_diagnoses_dict[f'noteevents_{patient2idx[patient_info]}.txt'] = parent\n",
    "            print('there')\n",
    "        else:\n",
    "            print(parent)\n",
    "    \n",
    "primary_diagnoses_dict = pd.DataFrame(list(primary_diagnoses_dict.values()), index=list(primary_diagnoses_dict.keys()), columns=['Primary ICD code'])\n",
    "\n",
    "print(primary_diagnoses_dict.head())\n",
    "#split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(primary_diagnoses_dict, test_size=0.2, stratify=primary_diagnoses_dict['Primary ICD code'])\n",
    "with open(f'./Data/downstream_datasets/Primary_diagnoses/df_train.pkl', 'wb') as handle:\n",
    "    pickle.dump(df_train, handle)\n",
    "with open(f'./Data/downstream_datasets/Primary_diagnoses/df_test.pkl', 'wb') as handle:\n",
    "    pickle.dump(df_test, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code in this section taken from github repo\n",
    "import spacy\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.matutils import sparse2full\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "#text2vec methods\n",
    "class text2vec():\n",
    "    def __init__(self, doc_list):\n",
    "        #Initialize\n",
    "        self.doc_list = doc_list\n",
    "        self.nlp, self.docs, self.docs_dict = self._preprocess(self.doc_list)\n",
    "    \n",
    "    # Functions to lemmatise docs\n",
    "    def _keep_token(self, t):\n",
    "        return (t.is_alpha and \n",
    "                not (t.is_space or t.is_punct or \n",
    "                     t.is_stop or t.like_num))\n",
    "    def _lemmatize_doc(self, doc):\n",
    "        return [ t.lemma_ for t in doc if self._keep_token(t)]\n",
    "\n",
    "\n",
    "    #Gensim to create a dictionary and filter out stop and infrequent words (lemmas).\n",
    "    def _get_docs_dict(self, docs):\n",
    "        docs_dict = Dictionary(docs)\n",
    "        #CAREFUL: For small corpus please carefully modify the parameters for filter_extremes, or simply comment it out.\n",
    "        #docs_dict.filter_extremes(no_below=2, no_above=0.5)\n",
    "        docs_dict.compactify()\n",
    "        return docs_dict\n",
    "\n",
    "    # Preprocess docs\n",
    "    def _preprocess(self, doc_list):\n",
    "#         #Load spacy model\n",
    "#         nlp  = spacy.load('en')\n",
    "#         #lemmatise docs\n",
    "#         docs = [self._lemmatize_doc(nlp(doc)) for doc in doc_list] \n",
    "#         print(docs)\n",
    "        #GET TOKENIZED DATA\n",
    "        \n",
    "        #Get docs dictionary\n",
    "        docs_dict = self._get_docs_dict(doc_list)\n",
    "        nlp = None\n",
    "        return nlp, doc_list, docs_dict\n",
    "\n",
    "\n",
    "    # Gensim can again be used to create a bag-of-words representation of each document,\n",
    "    # build the TF-IDF model, \n",
    "    # and compute the TF-IDF vector for each document.\n",
    "    def _get_tfidf(self, docs, docs_dict):\n",
    "        docs_corpus = [docs_dict.doc2bow(doc) for doc in docs]\n",
    "        model_tfidf = TfidfModel(docs_corpus, id2word=docs_dict)\n",
    "        docs_tfidf  = model_tfidf[docs_corpus]\n",
    "        docs_vecs   = np.vstack([sparse2full(c, len(docs_dict)) for c in docs_tfidf])\n",
    "        return docs_vecs\n",
    "\n",
    "\n",
    "    #Get avg w2v for one document\n",
    "    def _document_vector(self, doc, docs_dict, nlp):\n",
    "        # remove out-of-vocabulary words\n",
    "        doc_vector = [nlp(word).vector for word in doc if word in docs_dict.token2id]\n",
    "        return np.mean(doc_vector, axis=0)\n",
    "\n",
    "\n",
    "    # Get a TF-IDF weighted Glove vector summary for document list\n",
    "    # Input: a list of documents, Output: Matrix of vector for all the documents\n",
    "    def tfidf_weighted_wv(self):\n",
    "        model = fasttext.load_model('./Data/word_models/BioWordVec_PubMed_MIMICIII_d200.bin')\n",
    "        #tf-idf\n",
    "        docs_vecs   = self._get_tfidf(self.docs, self.docs_dict)\n",
    "\n",
    "        #Load glove embedding vector for each TF-IDF term\n",
    "        tfidf_emb_vecs = np.vstack([model.get_word_vector(self.docs_dict[i]) for i in range(len(self.docs_dict))])\n",
    "\n",
    "        #To get a TF-IDF weighted Glove vector summary of each document, \n",
    "        #we just need to matrix multiply docs_vecs with tfidf_emb_vecs\n",
    "        docs_emb = np.dot(docs_vecs, tfidf_emb_vecs)\n",
    "\n",
    "        return docs_emb\n",
    "\n",
    "    # Get average vector for document list\n",
    "    def avg_wv(self):\n",
    "        docs_vecs = np.vstack([self._document_vector(doc, self.docs_dict, self.nlp) for doc in self.docs])\n",
    "        return docs_vecs\n",
    "\n",
    "    # Get TF-IDF vector for document list\n",
    "    def get_tfidf(self):\n",
    "        docs_corpus = [self.docs_dict.doc2bow(doc) for doc in self.docs]\n",
    "        model_tfidf = TfidfModel(docs_corpus, id2word=self.docs_dict)\n",
    "        docs_tfidf  = model_tfidf[docs_corpus]\n",
    "        docs_vecs   = np.vstack([sparse2full(c, len(self.docs_dict)) for c in docs_tfidf])\n",
    "        return docs_vecs\n",
    "\n",
    "\n",
    "    # Get Latent Semantic Indexing(LSI) vector for document list\n",
    "    def get_lsi(self, num_topics=300):\n",
    "        docs_corpus = [self.docs_dict.doc2bow(doc) for doc in self.docs]\n",
    "        model_lsi = models.LsiModel(docs_corpus, num_topics, id2word=self.docs_dict)\n",
    "        docs_lsi  = model_lsi[docs_corpus]\n",
    "        docs_vecs   = np.vstack([sparse2full(c, len(self.docs_dict)) for c in docs_lsi])\n",
    "        return docs_vecs\n",
    "\n",
    "    # Get Random Projections(RP) vector for document list\n",
    "    def get_rp(self):\n",
    "        docs_corpus = [self.docs_dict.doc2bow(doc) for doc in self.docs]\n",
    "        model_rp = models.RpModel(docs_corpus, id2word=self.docs_dict)\n",
    "        docs_rp  = model_rp[docs_corpus]\n",
    "        docs_vecs   = np.vstack([sparse2full(c, len(self.docs_dict)) for c in docs_rp])\n",
    "        return docs_vecs\n",
    "\n",
    "    # Get Latent Dirichlet Allocation(LDA) vector for document list\n",
    "    def get_lda(self, num_topics=100):\n",
    "        docs_corpus = [self.docs_dict.doc2bow(doc) for doc in self.docs]\n",
    "        model_lda = models.LdaModel(docs_corpus, num_topics, id2word=self.docs_dict)\n",
    "        docs_lda  = model_lda[docs_corpus]\n",
    "        docs_vecs   = np.vstack([sparse2full(c, len(self.docs_dict)) for c in docs_lda])\n",
    "        return docs_vecs\n",
    "\n",
    "    # Get Hierarchical Dirichlet Process(HDP) vector for document list\n",
    "    def get_hdp(self):\n",
    "        docs_corpus = [self.docs_dict.doc2bow(doc) for doc in self.docs]\n",
    "        model_hdp = models.HdpModel(docs_corpus, id2word=self.docs_dict)\n",
    "        docs_hdp  = model_hdp[docs_corpus]\n",
    "        docs_vecs   = np.vstack([sparse2full(c, len(self.docs_dict)) for c in docs_hdp])\n",
    "        return docs_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obesity performance\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE\n",
    "from xml.dom import minidom\n",
    "import os\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from imblearn.over_sampling import BorderlineSMOTE as mSmote, SVMSMOTE , ADASYN, RandomOverSampler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "import scipy\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "doc2vec_model = Doc2Vec.load('./Data/doc2vec/doc2vec.bin')\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "scaler = StandardScaler()\n",
    "# feature_selection = SelectKBest(mutual_info_classif, k=400)   \n",
    "# feature_selection = LDA(n_components=500)\n",
    "# feature_selection = PCA(n_components=500)\n",
    "feature_selection = Isomap(n_components=100)\n",
    "#coumpute features\n",
    "discharge_summary_tokenized_dict = pickle.load(open(f'./Data/downstream_datasets/Obesity/discharge_summaries_tokenized_clamp.pkl', 'rb'))\n",
    "filenames = list(discharge_summary_tokenized_dict.keys())\n",
    "# doc_sentence_tokens = list(discharge_summary_tokenized_dict.values())\n",
    "# doc_word_tokens = [[y for x in doc_sentences for y in x] for doc_sentences in doc_sentence_tokens]\n",
    "# t2v = text2vec(doc_word_tokens)\n",
    "# feature_vecs = t2v.get_tfidf()\n",
    "# feature_dict = {filename:vec for filename,vec in zip(filenames,feature_vecs)}\n",
    "\n",
    "#computer features using my approach\n",
    "feature_dict = pickle.load(open('./Data/downstream_datasets/Obesity/document_embeddings_trans_flat_w_label_emb.pkl', 'rb'))\n",
    "# feature_dict_mimic = pickle.load(open('./Data/document_embeddings_trans_word_emb_train.pkl', 'rb'))\n",
    "# feature_dict.update(feature_dict_mimic)\n",
    "# feature_dict = {filename:doc2vec_model.infer_vector([filename], alpha=0.025, steps=100) for filename in filenames}\n",
    "# feature_dict = pickle.load(open(f'./Data/downstream_datasets/Obesity/sent2vec_mod_dict.pkl', 'rb'))\n",
    "# feature_dict = pickle.load(open(f'./Data/downstream_datasets/Obesity/model_results.pkl', 'rb'))\n",
    "# feature_dict = pickle.load(open(f'./Data/ctakes_vectors_word_emb_train.pkl', 'rb'))\n",
    "\n",
    "\n",
    "comorbidities = ['Asthma', 'CAD', 'CHF', \n",
    "               'Depression', 'Diabetes', 'Gallstones', 'GERD', 'Gout', \n",
    "               'Hypercholesterolemia', 'Hypertension', 'Hypertriglyceridemia', \n",
    "               'OA', 'Obesity', 'OSA', 'PVD', 'Venous Insufficiency']\n",
    "# comorbidities = ['OSA',]\n",
    "AUC = []\n",
    "PRFS = []\n",
    "\n",
    "root = minidom.Document()\n",
    "diseaseset = root.createElement('diseaseset')\n",
    "root.appendChild(diseaseset)\n",
    "\n",
    "diseasesetChild = root.createElement('diseases')\n",
    "diseasesetChild.setAttribute('source', 'intuitive')\n",
    "diseaseset.appendChild(diseasesetChild)\n",
    "\n",
    "sm = mSmote(random_state=42, k_neighbors = 3)\n",
    "\n",
    "for comorbidity in comorbidities:\n",
    "    print(comorbidity)\n",
    "    #train\n",
    "    df_train = pickle.load(open(f'./Data/downstream_datasets/Obesity/df_train.pkl', 'rb'))\n",
    "    df_train = df_train.set_index('Obesity_train_'+df_train.index.astype(str)+'.txt')\n",
    "    df_train_add = pickle.load(open(f'./Data/downstream_datasets/Obesity/df_train_add.pkl', 'rb'))\n",
    "    \n",
    "    df_train = df_train[(df_train[comorbidity].isin(['Y', 'N']))]\n",
    "#     df_train = sample(df_train, df_train_add, comorbidity, k=2)\n",
    "    \n",
    "    #     print('len of train before: ', len(df_train))\n",
    "    filenames_w_features = [filename for filename in list(df_train.index) if feature_dict.get(filename) is not None]\n",
    "    df_train = df_train.filter(items=filenames_w_features, axis=0)\n",
    "#     print('len of train after: ', len(df_train))\n",
    "    #get labels and filenames\n",
    "    \n",
    "    print(df_train[comorbidity].value_counts())\n",
    "    labels = df_train[comorbidity].values\n",
    "    #make label coder on training\n",
    "    le.fit(labels)\n",
    "#     print(le.classes_)\n",
    "    if(len(le.classes_) < 2):\n",
    "        continue\n",
    "    labels = le.transform(labels) \n",
    "    filenames = list(df_train.index)\n",
    "    features = [feature_dict[filename]for filename in filenames]# if feature_dict.get(f'Obesity_train_{filename}.txt') is not None]\n",
    "    features = np.vstack(features)\n",
    "    print('Train Size: ', len(features))\n",
    "    features = scaler.fit_transform(features)\n",
    "#     features = feature_selection.fit_transform(features, labels)\n",
    "#     features_res, labels_res = sm.fit_resample(features, labels)\n",
    "\n",
    "#     model = LogisticRegression(class_weight='balanced').fit(features,labels)\n",
    "#     model = LogisticRegression(C=100,class_weight='balanced').fit(features,labels)\n",
    "#     best_params = pickle.load(open(f'./Models/best_params/i2b2_{comorbidity}.pkl', 'rb'))\n",
    "#     model = LogisticRegression(**best_params).fit(features,labels)\n",
    "#     model = SVC(C=0.01, kernel='linear', class_weight='balanced').fit(features,labels)\n",
    "#     model = SVC(kernel='linear').fit(features_res,labels_res)\n",
    "#     model = RandomForestClassifier().fit(features,labels)\n",
    "#     model = BalancedRandomForestClassifier(class_weight='balanced').fit(features,labels)\n",
    "#     model = RFE(LogisticRegression(class_weight='balanced'), 500).fit(features,labels)\n",
    "    distributions = {'C': loguniform(1e-4,1e-1), 'penalty':['l1', 'l2'], 'max_iter':loguniform(1e1, 1e3), 'tol':loguniform(1e-4,1e-1), \n",
    "                     'class_weight':['balanced', None], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "    model = RandomizedSearchCV(LogisticRegression(), distributions, random_state=0).fit(features,labels)\n",
    "#     distributions = {'C': loguniform(1e-4,1e-1), 'gamma':loguniform(1e-4,1e-1), 'max_iter':loguniform(1e1, 1e3), 'tol':loguniform(1e-4,1e-1), \n",
    "#                      'class_weight':['balanced', None], 'shrinking': [True, False]}\n",
    "#     model = RandomizedSearchCV(SVC(kernel='linear'), distributions, random_state=0).fit(features,labels)\n",
    "#     with open(f'./Models/best_params/i2b2_{comorbidity}.pkl', 'wb') as handle:\n",
    "#         pickle.dump(model.best_params_, handle)\n",
    "    \n",
    "    #train perf \n",
    "    y_pred = model.predict(features)\n",
    "    print(classification_report(labels, y_pred))\n",
    "    \n",
    "    \n",
    "    #test\n",
    "    df_test = pickle.load(open(f'./Data/downstream_datasets/Obesity/df_test.pkl', 'rb'))\n",
    "    filenames_w_features = [filename.split('.')[0].split('_')[-1] for filename in feature_dict.keys() if 'Obesity_test' in filename]\n",
    "    df_test = df_test.filter(items=filenames_w_features, axis=0)\n",
    "    df_test = df_test[(df_test[comorbidity].isin(['Y', 'N','Q']))]\n",
    "#     labels = df_test[comorbidity].values\n",
    "#     labels = le.transform(labels)\n",
    "#     n_labels = len(np.unique(labels))\n",
    "#     print('number of labels: ', n_labels)\n",
    "    filenames = list(df_test.index)\n",
    "#     print(filenames)\n",
    "    features = [feature_dict[f'Obesity_test_{filename}.txt']for filename in filenames]# if feature_dict.get(f'Obesity_test_{filename}.txt') is not None]\n",
    "    features = np.vstack(features)\n",
    "    print('Test Size: ', len(features))\n",
    "    features = scaler.transform(features)\n",
    "#     features = feature_selection.transform(features)\n",
    "    print(features.shape)\n",
    "    \n",
    "    y_pred = model.predict(features)\n",
    "    \n",
    "    diseasesChild = root.createElement('disease')\n",
    "    diseasesChild.setAttribute('name', comorbidity)\n",
    "    diseasesetChild.appendChild(diseasesChild)\n",
    "    for filename, y_pred_i in sorted(zip(filenames, le.inverse_transform(y_pred)), key=lambda x:int(x[0])):\n",
    "        predictionElem = root.createElement('doc')\n",
    "        predictionElem.setAttribute('id', filename)\n",
    "        predictionElem.setAttribute('judgment', str(y_pred_i))\n",
    "        diseasesChild.appendChild(predictionElem)\n",
    "#     print(classification_report(labels, y_pred))\n",
    "#     print('auc: ', roc_auc_score(labels, y_pred))\n",
    "#     AUC.append(roc_auc_score(labels, y_pred))\n",
    "#     print('prf ', precision_recall_fscore_support(labels, y_pred, labels=[1,]))\n",
    "#     print('prf ', precision_recall_fscore_support(labels, y_pred, average='macro'))\n",
    "#     elem = list(precision_recall_fscore_support(labels, y_pred, average='macro'))\n",
    "#     elem.append(n_labels)\n",
    "#     elem[3] = len(df_test)\n",
    "#     print(elem)\n",
    "#     PRFS.append(elem)\n",
    "\n",
    "# total_samples = sum([elem[3] for elem in PRFS])\n",
    "\n",
    "xml_str = root.toprettyxml(indent = \"\", newl = \"\\n\", encoding = \"utf-8\")\n",
    "\n",
    "with open('./Data/downstream_datasets/Obesity/test_han_bert.xml' , 'wb+') as f:\n",
    "    f.write(xml_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = sum([elem[3] for elem in PRFS])\n",
    "# macro_auc = sum(AUC)/len(AUC)\n",
    "# micro_auc = sum([auc*elem[3][0] for auc,elem in zip(AUC,PRFS)])/total_samples\n",
    "# print('AUC: ', macro_auc, micro_auc)\n",
    "macro_precision = sum([elem[0] for elem in PRFS])/len(PRFS)\n",
    "micro_precision = sum([elem[0]*elem[3] for elem in PRFS])/total_samples\n",
    "print('Precision: ', macro_precision,micro_precision)\n",
    "macro_recall = sum([elem[1] for elem in PRFS])/len(PRFS)\n",
    "micro_recall = sum([elem[1]*elem[0] for elem in PRFS])/total_samples\n",
    "print('Recall: ', macro_recall,micro_recall)\n",
    "macro_fscore = sum([elem[2] for elem in PRFS])/len(PRFS)\n",
    "micro_fscore = sum([elem[2]*elem[3] for elem in PRFS])/total_samples\n",
    "macro_fscore_new = sum([elem[2]*elem[-1] for elem in PRFS])/sum([elem[-1] for elem in PRFS])\n",
    "print('Fscore: ', macro_fscore,macro_fscore_new,micro_fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.svm import OneClassSVM \n",
    "from sklearn.decomposition import PCA,TruncatedSVD\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2 , mutual_info_classif\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.utils.fixes import loguniform\n",
    "import scipy\n",
    "\n",
    "class ModelPerf():\n",
    "    def __init__(self, task='Morbidity', multiclass=False): #taks : morbidity or primary diagnoses prediction\n",
    "        self.task = task\n",
    "        self.t2v = None\n",
    "#         self.feature_dict = self.comput_features_doc2vec()\n",
    "#         self.feature_dict = self.compute_features() \n",
    "        self.feature_dict = pickle.load(open(f'./Data/document_embeddings_trans_with_patient_notes_1.pkl', 'rb'))\n",
    "        self.feature_dict_ref = pickle.load(open(f'./Data/ctakes_vectors_word_emb_train.pkl', 'rb'))\n",
    "#         self.feature_dict = pickle.load(open(f'./Data/model_results.pkl', 'rb'))\n",
    "#         self.feature_dict = pickle.load(open(f'./Data/sent2vec_dict.pkl', 'rb'))\n",
    "#         self.feature_dict = pickle.load(open(f'./Data/bert_dict.pkl', 'rb'))\n",
    "        self.le = preprocessing.LabelEncoder()\n",
    "        self.scaler = PowerTransformer(method = 'yeo-johnson')\n",
    "#         self.feature_selection = SelectFromModel(LogisticRegression(C=1, penalty='l1', solver='liblinear'))\n",
    "        self.feature_selection = SelectKBest(mutual_info_classif, k=500)\n",
    "        self.dim_reducer = TruncatedSVD(n_components=100, random_state=20)\n",
    "        self.model = None\n",
    "        self.rfe = None\n",
    "        self.multiclass = multiclass\n",
    "        self.train()\n",
    "        self.test()\n",
    "#         self.find_imp_features()\n",
    "        \n",
    "    def compute_features(self):\n",
    "        ps = PorterStemmer() \n",
    "        discharge_summary_tokenized_dict = pickle.load(open(f'./Data/downstream_datasets/{self.task}/discharge_summaries_tokenized_clamp.pkl', 'rb'))\n",
    "        filenames = list(discharge_summary_tokenized_dict.keys())\n",
    "        doc_sentence_tokens = list(discharge_summary_tokenized_dict.values())\n",
    "        doc_word_tokens = [[y for x in doc_sentences for y in x] for doc_sentences in doc_sentence_tokens]\n",
    "        self.t2v = text2vec(doc_word_tokens)\n",
    "        feature_vecs = self.t2v.get_tfidf()\n",
    "        feature_dict = {filename:vec for filename,vec in zip(filenames,feature_vecs)}\n",
    "        return feature_dict\n",
    "    \n",
    "    def comput_features_doc2vec(self):\n",
    "        doc2vec_model = Doc2Vec.load('./Data/doc2vec/doc2vec.bin')\n",
    "        discharge_summary_tokenized_dict = pickle.load(open(f'./Data/downstream_datasets/{self.task}/discharge_summaries_tokenized_clamp.pkl', 'rb'))\n",
    "        filenames = list(discharge_summary_tokenized_dict.keys())\n",
    "        feature_dict = {filename:doc2vec_model.infer_vector([filename], alpha=0.025, steps=100) for filename in filenames}\n",
    "        return feature_dict\n",
    "    \n",
    "    def get_features(self, filenames, mode='train'):\n",
    "        features = [self.feature_dict[filename] for filename in filenames]\n",
    "    \n",
    "    def get_labels(self, df, mode='train'):   \n",
    "        labels = df['12'].values\n",
    "        if(mode == 'train'):\n",
    "            self.le.fit(labels)\n",
    "            print(self.le.classes_)\n",
    "        labels = self.le.transform(labels)\n",
    "        filenames = list(df.index)\n",
    "        return labels, filenames\n",
    "    \n",
    "    def train(self):\n",
    "        df_train = pickle.load(open(f'./Data/downstream_datasets/{self.task}/df_train.pkl', 'rb'))\n",
    "        filenames_w_features = list(set(list(self.feature_dict_ref.keys())).intersection(set(list(self.feature_dict.keys()))))\n",
    "        df_train = df_train[df_train.index.isin(filenames_w_features)]\n",
    "        print(len(df_train))\n",
    "        labels, filenames = self.get_labels(df_train)\n",
    "        features = self.get_features(filenames)\n",
    "        print(features.shape)\n",
    "#         features = self.scaler.fit_transform(features)\n",
    "#         features = self.feature_selection.fit_transform(features, labels)\n",
    "#         features = self.dim_reducer.fit_transform(features)\n",
    "        if(self.multiclass):\n",
    "            self.model = LogisticRegression(class_weight='balanced', multi_class='ovr').fit(features,labels)\n",
    "        else:\n",
    "#             self.model = LogisticRegression().fit(features,labels)\n",
    "#             self.model = LogisticRegression(**pickle.load(open('./Models/best_params/1Y.pkl', 'rb'))).fit(features,labels)\n",
    "            distributions = {'C': loguniform(1e-4,2), 'penalty':['l1', 'l2'], 'max_iter':loguniform(1e1, 1e3), 'tol':loguniform(1e-5,1e-1), \n",
    "                     'class_weight':['balanced', None], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "            self.model = RandomizedSearchCV(LogisticRegression(), distributions, random_state=0).fit(features,labels)\n",
    "#             self.model = LinearSVC(class_weight='balanced').fit(features,labels)\n",
    "#             self.model = RandomForestClassifier(class_weight='balanced', random_state=15325).fit(features,labels)\n",
    "#             self.model = BalancedRandomForestClassifier().fit(features,labels)\n",
    "#             self.model = OneClassSVM(nu=861/21308, gamma=0.001).fit(features)\n",
    "    \n",
    "    def test(self):\n",
    "        df_test = pickle.load(open(f'./Data/downstream_datasets/{self.task}/df_test.pkl', 'rb'))\n",
    "        filenames_w_features = list(set(list(self.feature_dict_ref.keys())).intersection(set(list(self.feature_dict.keys()))))\n",
    "        df_test = df_test[df_test.index.isin(filenames_w_features)]\n",
    "        print(len(df_test))\n",
    "        labels, filenames = self.get_labels(df_test, mode='test')\n",
    "        features = self.get_features(filenames, mode='test')\n",
    "#         features = self.dim_reducer.transform(features)\n",
    "#         features = self.scaler.transform(features)\n",
    "#         features = self.feature_selection.transform(features)\n",
    "        y_pred = self.model.predict(features)\n",
    "        print(classification_report(labels, y_pred))\n",
    "        print('auc: ', roc_auc_score(labels, self.model.predict_proba(features)[:,1])*100)\n",
    "        print('AP:', average_precision_score(labels, self.model.predict_proba(features)[:,1])*100)\n",
    "        print(precision_recall_fscore_support(labels, y_pred, average='macro'))\n",
    "        print(precision_recall_fscore_support(labels, y_pred, average='micro'))\n",
    "        \n",
    "    def find_imp_features(self):\n",
    "        df_train = pickle.load(open(f'./Data/downstream_datasets/{self.task}/df_train.pkl', 'rb'))\n",
    "        labels, filenames = self.get_labels(df_train)\n",
    "        features = self.get_features(filenames)\n",
    "#         features = self.scaler.fit_transform(features)\n",
    "        model = LogisticRegression(class_weight='balanced')\n",
    "        self.rfe = RFE(model, 100)\n",
    "        self.rfe = self.rfe.fit(features, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
